{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "# αβαθούλωτος, the table tag contains the declension of said adjective. Nouns follow the same form\n",
    "\n",
    "url = \"https://el.wiktionary.org/wiki/%CE%B1%CE%B2%CE%B3%CF%8C\"\n",
    "response = http.request('GET', url)\n",
    "soup = BeautifulSoup(response.data,'lxml')\n",
    "filename= soup.title.string\n",
    "filename\n",
    "i= 0\n",
    "l = []\n",
    "#for t in soup.table.find_all('td'):\n",
    "for cell in soup.select('td'):\n",
    "    \n",
    "    row = cell.text\n",
    "    l.append(row)\n",
    "    \n",
    "    \n",
    "    #print(type(cell.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ονομαστική\\n',\n",
       " 'το\\n',\n",
       " 'αβγό\\n',\n",
       " 'τα\\n',\n",
       " 'αβγά\\n',\n",
       " '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0γενική\\n',\n",
       " 'του\\n',\n",
       " 'αβγού\\n',\n",
       " 'των\\n',\n",
       " 'αβγών\\n',\n",
       " '\\xa0\\xa0\\xa0\\xa0αιτιατική\\n',\n",
       " 'το\\n',\n",
       " 'αβγό\\n',\n",
       " 'τα\\n',\n",
       " 'αβγά\\n',\n",
       " '\\xa0\\xa0\\xa0\\xa0\\xa0κλητική\\n',\n",
       " '\\n',\n",
       " 'αβγό\\n',\n",
       " '\\n',\n",
       " 'αβγά\\n',\n",
       " 'Παράρτημα:Ουσιαστικά\\n',\n",
       " '\\nαρχαία ελληνικά\\xa0: ᾠόν\\nαγγλικά\\xa0: egg (en)\\nαραβικά\\xa0: بيضة (ar)\\nαρμενικά\\xa0: ձու (hy)\\nβουλγαρικά\\xa0: яйце (bg)\\nγαλλικά\\xa0: œuf (fr)\\nγερμανικά\\xa0: Ei (de)\\nδανικά\\xa0: æg (da)\\nεβραϊκά\\xa0: ביצה (he)\\nεσθονικά\\xa0: muna (et)\\nεσπεράντο\\xa0: ovo (eo)\\nιαπωνικά\\xa0: 卵 (ja) (たまご, tamago)\\nιντερλίνγκουα\\xa0: ovo (ia)\\nίντο\\xa0: ovo (io)\\nισλανδικά\\xa0: egg (is)\\nισπανικά\\xa0: huevo (es)\\nιταλικά\\xa0: uovo (it)\\nκαζακικά\\xa0: жұмыртқа (kk)\\nκαταλανικά\\xa0: ou (ca)\\nκινεζικά\\xa0: 蛋 (zh) (dàn), 卵 (zh) (luǎn), 鸡蛋 (zh) (jīdàn) (κότας)\\nκροατικά\\xa0: jaje (hr)\\n',\n",
       " '\\n',\n",
       " '\\nλατινικά\\xa0: ovum (la)\\nλιθουανικά\\xa0: kiaušinis (lt)\\nμαλαϊκά\\xa0: telur (ms)\\nνορβηγικά\\xa0: egg (no)\\nολλανδικά\\xa0: ei (nl)\\nοξιτανικά\\xa0: uòu (oc)\\nουαλικά\\xa0: wy (cy)\\nουγγρικά\\xa0: tojás (hu)\\nουκρανικά\\xa0: яйце (uk)\\nπολωνικά\\xa0: jajko (pl)\\nπορτογαλικά\\xa0: ovo (pt)\\nρουμανικά\\xa0: ou (ro)\\nρωσικά\\xa0: яйцо (ru) (yaitsó), икринка (ru)\\nσλοβακικά\\xa0: vajce (sk)\\nσλοβενικά\\xa0: jajce (sl)\\nσουηδικά\\xa0: ägg (sv)\\nτουρκικά\\xa0: yumurta (tr), tohum (tr)\\nτσεχικά\\xa0: vejce (cs), vajíčko (cs)\\nφινλανδικά\\xa0: muna (fi)\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'beautify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-665ae5d1e171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://el.wiktionary.org/wiki/%CE%B1%CE%B2%CE%B1%CE%B8%CE%BF%CF%8D%CE%BB%CF%89%CF%84%CE%BF%CF%82'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeautify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'beautify'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "from lxml import etree\n",
    "html = requests.get('https://el.wiktionary.org/wiki/%CE%B1%CE%B2%CE%B1%CE%B8%CE%BF%CF%8D%CE%BB%CF%89%CF%84%CE%BF%CF%82')\n",
    "doc = lxml.html.fromstring(html.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "url = 'https://el.wiktionary.org/wiki/%CE%B1%CE%B2%CE%B1%CE%B8%CE%BF%CF%8D%CE%BB%CF%89%CF%84%CE%BF%CF%82'\n",
    "html = urlopen(url) \n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'child'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6962517a857d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 1807\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'child'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "soup = soup.child['body']\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.spiders import CrawlSpider\n",
    "from scrapy.spiders import Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "from scrapy import Item\n",
    "from scrapy import Field\n",
    "\n",
    "\n",
    "class UrlItem(Item):\n",
    "    url = Field()\n",
    "\n",
    "\n",
    "class WikiSpider(CrawlSpider):\n",
    "    name = 'wiki'\n",
    "    allowed_domains = ['el.wiktionary.org']\n",
    "    start_urls = ['https://el.wiktionary.org/wiki/%CE%92%CE%B9%CE%BA%CE%B9%CE%BB%CE%B5%CE%BE%CE%B9%CE%BA%CF%8C:%CE%9A%CF%8D%CF%81%CE%B9%CE%B1_%CE%A3%CE%B5%CE%BB%CE%AF%CE%B4%CE%B1']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(), callback='parse_url'),\n",
    "    )\n",
    "\n",
    "    def parse_url(self, response):\n",
    "        item = UrlItem()\n",
    "        item['url'] = response.url\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-5c6f4b704f94>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-5c6f4b704f94>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    scrapy crawl wiki -o wiki.csv -t csv\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy crawl wiki -o wiki.csv -t csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://el.wiktionary.org/wiki/%CE%9A%CE%B1%CF%84%CE%B7%CE%B3%CE%BF%CF%81%CE%AF%CE%B1:%CE%95%CE%BB%CE%BB%CE%B7%CE%BD%CE%B9%CE%BA%CE%AE_%CE%B3%CE%BB%CF%8E%CF%83%CF%83%CE%B1\"\n",
    "\n",
    "soup = BeautifulSoup(http(URL))\n",
    "links = soup.select('.topline > a')\n",
    "for a in links:\n",
    "    link = link.get('href')\n",
    "    if link:\n",
    "        # follow link\n",
    "        link_soup = BeautifulSoup(urlopen(link))\n",
    "        title = link_soup.find('title')\n",
    "        # check title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import colorama\n",
    "\n",
    "# init the colorama module\n",
    "colorama.init()\n",
    "\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "\n",
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "total_urls_visited = 0\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://el.wiktionary.org/wiki/%CE%9A%CE%B1%CF%84%CE%B7%CE%B3%CE%BF%CF%81%CE%AF%CE%B1:%CE%9F%CF%85%CF%83%CE%B9%CE%B1%CF%83%CF%84%CE%B9%CE%BA%CE%AC_(%CE%BD%CE%AD%CE%B1_%CE%B5%CE%BB%CE%BB%CE%B7%CE%BD%CE%B9%CE%BA%CE%AC)'\n",
    "links = get_all_website_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pages = set()\n",
    "\n",
    "def get_links(page_url):\n",
    "    global pages\n",
    "    pattern = re.compile(\"^(/)\")\n",
    "    html = requests.get(page_url).text # fstrings require Python 3.6+\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for link in soup.find_all(\"a\", href=pattern):\n",
    "        if \"href\" in link.attrs:\n",
    "            if link.attrs[\"href\"] not in pages:\n",
    "                new_page = link.attrs[\"href\"]\n",
    "                print(new_page)\n",
    "                pages.add(new_page)\n",
    "                get_links(new_page)\n",
    "        \n",
    "l=get_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=crawl(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_href = urlparse(href)\n",
    "# remove URL GET parameters, URL fragments, etc.\n",
    "href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_href.scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
